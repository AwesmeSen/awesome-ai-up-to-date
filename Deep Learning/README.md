## Bayesian Deep Learning
- Dec 30, 2019 - [The Case for Bayesian Deep Learning](https://cims.nyu.edu/~andrewgw/caseforbdl.pdf)

## Hyperparameter Tuning
- Jan 10, 2020 - [Sweeps by W&B - Hyperparameter tuning as easy as 1-2-3](https://www.wandb.com/articles/hyperparameter-tuning-as-easy-as-1-2-3)

## Experimentation Tracking
- [Weights & Biases](https://www.wandb.com/)

## Inference
- [ONNX Runtime: cross-platform, high performance scoring engine for ML models ](https://github.com/Microsoft/onnxruntime)

## Loss functions
- Mar 7, 2018 - [Investigating focal and dice loss](https://becominghuman.ai/investigating-focal-and-dice-loss-for-the-kaggle-2018-data-science-bowl-65fb9af4f36c)

## Optimizer
- Dec 5, 2019 - [Deep Double Descent](https://openai.com/blog/deep-double-descent/)
- Jan 5, 2020 - Blog - [AdaMod - Optimizer with memory](https://medium.com/@lessw/meet-adamod-a-new-deep-learning-optimizer-with-memory-f01e831b80bd)

## Post-rationalization
- Dec 5, 2019 - Blog - [Deep Double Descent](https://openai.com/blog/deep-double-descent/)

## Interpretability
- Jan 15, 2020 - [Featuring the impact of feature attribution baselines](https://distill.pub/2020/attribution-baselines/)

## Trends Overview
- Jan 10, 2020 - [Deep Learning State of the Art 2020 - MIT](https://www.youtube.com/watch?v=0VH1Lim8gL8)
- Jan 1, 2020 - [AI in 2020 - Connor Shorten](https://www.youtube.com/watch?v=6SWpN64Ivb4&feature=youtu.be)
- Dec 30, 2019 - [A birdâ€™s-eye view of modern AI from NeurIPS 2019](https://alexkolchinski.com/2019/12/30/neurips-2019/)
- Dec 18, 2019 - [Key Trends from NeurIPS 2019](https://huyenchip.com/2019/12/18/key-trends-neurips-2019.html)

## Neural ODEs
- Jan 14, 2020 - [How To Train Interpretable Neural Networks That Accurately Extrapolate From Small Data](https://www.stochasticlifestyle.com/how-to-train-interpretable-neural-networks-that-accurately-extrapolate-from-small-data/)

## Attention
- Nov 15, 2019 - [How a self-attention layer learn convolutional filters?](http://jbcordonnier.com/posts/attention-cnn/) [Code](https://github.com/epfml/attention-cnn)

## Invariant Learning
- Jun 2019 - [On Learning Invariant Representations for Domain Adaptation](https://blog.ml.cmu.edu/2019/09/13/on-learning-invariant-representations-for-domain-adaptation/)

## Pruning
- Feb 8, 2020 - [Soft Threshold Weight Reparameterization for Learnable Sparsity](https://arxiv.org/abs/2002.03231)
- Jan 15, 2020 - [A "Network Pruning Network" Approach to Deep Model Compression](https://arxiv.org/abs/2001.05545)

## Generalization
- Jan 14, 2020 - [Understanding Generalization in Deep Learning via Tensor Methods](https://arxiv.org/abs/2001.05070)


## Neural Tangent
- Feb 10, 2020 - [Neural Tangents: Fast and Easy Infinite Neural Networks in Python](https://arxiv.org/abs/1912.02803)

## Lattice based controllable learning
- Feb 10, 2020 - [TensorFlow Lattice: Flexible, controlled and interpretable ML](https://blog.tensorflow.org/2020/02/tensorflow-lattice-flexible-controlled-and-interpretable-ML.html?linkId=82088125)